{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAOgodntxXe5"
   },
   "outputs": [],
   "source": [
    "# Run this cell first and upload your kaggle.json file\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Please upload your kaggle.json file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move kaggle.json to the correct directory\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"Kaggle API token configured successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UN21hnvs4OID"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 1. Download the dataset\n",
    "dataset_slug = \"asaniczka/tmdb-movies-dataset-2023-930k-movies\"\n",
    "print(f\"Downloading dataset: {dataset_slug}...\")\n",
    "!kaggle datasets download -d {dataset_slug}\n",
    "\n",
    "# 2. Unzip the file\n",
    "zip_name = dataset_slug.split('/')[-1] + \".zip\"\n",
    "with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"tmdb_data\")\n",
    "\n",
    "# 3. Find the CSV file (automatically detects the name)\n",
    "csv_files = glob.glob(\"tmdb_data/*.csv\")\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\"No CSV file found in the downloaded dataset.\")\n",
    "csv_path = csv_files[0]\n",
    "print(f\"Reading file: {csv_path}\")\n",
    "\n",
    "# 4. Load data\n",
    "# specifying low_memory=False to suppress mixed type warnings on large files\n",
    "df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "# 5. Preprocessing & Filtering\n",
    "\n",
    "# Convert release_date to datetime, handling errors by coercing to NaT\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
    "\n",
    "# Drop rows with no release date\n",
    "df = df.dropna(subset=['release_date'])\n",
    "\n",
    "# Define Filter Criteria\n",
    "START_DATE = '2015-12-28'\n",
    "END_DATE = '2025-12-28'\n",
    "MIN_VOTES = 100\n",
    "MIN_REVENUE = 50_000_000  # 50 Million\n",
    "\n",
    "print(f\"Filtering for release date between {START_DATE} and {END_DATE}...\")\n",
    "print(f\"Filtering for vote_count > {MIN_VOTES}...\")\n",
    "print(f\"Filtering for revenue > {MIN_REVENUE}...\")\n",
    "\n",
    "# Apply Filters\n",
    "filtered_df = df[\n",
    "    (df['release_date'] >= START_DATE) &\n",
    "    (df['release_date'] <= END_DATE) &\n",
    "    (df['vote_count'] > MIN_VOTES) &\n",
    "    (df['revenue'] > MIN_REVENUE)\n",
    "].copy()\n",
    "\n",
    "print(f\"Movies remaining after filtering: {len(filtered_df)}\")\n",
    "\n",
    "# 6. Random Sampling\n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "if len(filtered_df) > SAMPLE_SIZE:\n",
    "    sampled_df = filtered_df.sample(n=SAMPLE_SIZE, random_state=42) # random_state for reproducibility\n",
    "    print(f\"Successfully sampled {SAMPLE_SIZE} movies.\")\n",
    "else:\n",
    "    sampled_df = filtered_df\n",
    "    print(f\"Warning: Only {len(filtered_df)} movies matched criteria. Returning all of them.\")\n",
    "\n",
    "# 7. Save to CSV\n",
    "output_filename = \"filtered_tmdb_movies_sample.csv\"\n",
    "sampled_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nDone! Saved to {output_filename}\")\n",
    "\n",
    "# Trigger download of the result\n",
    "files.download(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEE7nspw-szb"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Configuration\n",
    "cities = {\n",
    "    \"Tunis\":  {\"lat\": 36.8065, \"lon\": 10.1815},\n",
    "    \"Sousse\": {\"lat\": 35.8256, \"lon\": 10.6084},\n",
    "    \"Sfax\":   {\"lat\": 34.7406, \"lon\": 10.7603}\n",
    "}\n",
    "\n",
    "START_DATE = \"2023-12-20\"\n",
    "END_DATE = \"2025-12-20\"\n",
    "\n",
    "# 2. Helper Functions for Logic\n",
    "\n",
    "def get_weather_state(code):\n",
    "    \"\"\"\n",
    "    Maps WMO Weather Codes to General States.\n",
    "    Source: Open-Meteo WMO docs\n",
    "    \"\"\"\n",
    "    if code in [0, 1]:\n",
    "        return \"Clear\"\n",
    "    elif code in [2, 3]:\n",
    "        return \"Cloudy\"\n",
    "    elif code in [45, 48]:\n",
    "        return \"Foggy\"\n",
    "    elif code in [51, 53, 55, 61, 63, 65, 80, 81, 82]:\n",
    "        return \"Rainy\"\n",
    "    elif code in [71, 73, 75, 77, 85, 86]:\n",
    "        return \"Snow/Hail\" # Rare in Tunisia, but possible\n",
    "    elif code in [95, 96, 99]:\n",
    "        return \"Stormy\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def get_temp_category(temp):\n",
    "    \"\"\"\n",
    "    Categorizes temperature based on Tunisian averages.\n",
    "    \"\"\"\n",
    "    if temp < 13:\n",
    "        return \"Cold\"       # Good for cinema (Indoor)\n",
    "    elif 13 <= temp < 25:\n",
    "        return \"Mild\"       # Bad for cinema (People go outside)\n",
    "    elif 25 <= temp < 32:\n",
    "        return \"Warm\"       # Neutral\n",
    "    else:\n",
    "        return \"Hot\"        # Good for cinema (AC seeking)\n",
    "\n",
    "# 3. Main Loop to Fetch Data\n",
    "all_data = []\n",
    "\n",
    "print(f\"Fetching data from {START_DATE} to {END_DATE}...\\n\")\n",
    "\n",
    "for city_name, coords in cities.items():\n",
    "    print(f\"Processing {city_name}...\")\n",
    "\n",
    "    # Open-Meteo Archive API\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": coords[\"lat\"],\n",
    "        \"longitude\": coords[\"lon\"],\n",
    "        \"start_date\": START_DATE,\n",
    "        \"end_date\": END_DATE,\n",
    "        \"daily\": [\"weather_code\", \"temperature_2m_max\"],\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        daily = data['daily']\n",
    "\n",
    "        # Create a temporary dataframe for this city\n",
    "        df_city = pd.DataFrame({\n",
    "            'date': daily['time'],\n",
    "            'max_temp': daily['temperature_2m_max'],\n",
    "            'wmo_code': daily['weather_code']\n",
    "        })\n",
    "\n",
    "        df_city['city'] = city_name\n",
    "\n",
    "        # Apply Logic\n",
    "        df_city['weather_state'] = df_city['wmo_code'].apply(get_weather_state)\n",
    "        df_city['temp_category'] = df_city['max_temp'].apply(get_temp_category)\n",
    "\n",
    "        all_data.append(df_city)\n",
    "    else:\n",
    "        print(f\"Error fetching {city_name}: {response.status_code}\")\n",
    "\n",
    "# 4. Combine and Clean\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Select only the specific columns you requested\n",
    "output_df = final_df[['city', 'date', 'weather_state', 'temp_category']]\n",
    "\n",
    "# 5. Review and Save\n",
    "print(\"\\nSample Data:\")\n",
    "print(output_df.sample(10))\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv(\"tunisia_historical_weather.csv\", index=False)\n",
    "print(\"\\nSaved to 'tunisia_historical_weather.csv'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
